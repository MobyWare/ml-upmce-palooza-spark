{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest of Doom\n",
    "### By Ryan Dickson\n",
    "### Edited by Mohinder Dick\n",
    "> References go here for presentation version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from dateutil.parser import parse\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, HashingTF, VectorAssembler, IDF\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment depend variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file path: file:///C:/Users/dickm/Documents/Projects/ML/Source/UPMC/Pharmacy/visit_train_panda.csv.\n",
      "Test file path: file:///C:/Users/dickm/Documents/Projects/ML/Source/UPMC/Pharmacy/visit_test_panda.csv.\n",
      "Pyspark path: None.\n"
     ]
    }
   ],
   "source": [
    "# Set up the path of the files and environment va\n",
    "\n",
    "if sc._conf.get('spark.master') == 'yarn-client' or sc._conf.get('spark.master') == 'yarn-cluster':\n",
    "    urn = 'hdfs://sparkdl04:8020/palooza/data/visit_train_panda.csv'\n",
    "    urnTest = 'hdfs://sparkdl04:8020/palooza/data/validate/visit_test_panda.csv'\n",
    "    \n",
    "    # Set up the path of the python pyspark should use\n",
    "    #os.environ['PYSPARK_PYTHON'] = '/opt/anaconda/bin/python'\n",
    "else:\n",
    "    urn = 'file:///C:/Users/dickm/Documents/Projects/ML/Source/UPMC/Pharmacy/visit_train_panda.csv'\n",
    "    urnTest = 'file:///C:/Users/dickm/Documents/Projects/ML/Source/UPMC/Pharmacy/visit_test_panda.csv'\n",
    "    #os.environ['PYSPARK_PYTHON'] = 'C:\\Users\\dickm\\AppData\\Local\\Continuum\\Anaconda2\\python.exe'\n",
    "\n",
    "# Used avoid issues in windows when reading text files.\n",
    "partitionNumber = 500\n",
    "\n",
    "print('Train file path: %s.\\nTest file path: %s.\\nPyspark path: %s.' % (urn, urnTest, os.environ.get('PYSPARK_PYTHON')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark\n",
    "\n",
    "This launches the notebook and provides the spark context in the variable *sc*. You can use the context to preview the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'spark.executor.instances', u'6'), (u'spark.executor.memory', u'512m'), (u'spark.rdd.compress', u'True'), (u'spark.master', u'local[3]'), (u'spark.serializer.objectStreamReset', u'100'), (u'spark.submit.deployMode', u'client'), (u'spark.app.name', u'PySparkShell')]\n",
      "\n",
      "sum of 1 to 1000:  499500\n"
     ]
    }
   ],
   "source": [
    "print sc._conf.getAll()\n",
    "test = sc.parallelize(range(1000))\n",
    "print '\\nsum of 1 to 1000: ', test.reduce(lambda a, b: a+b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added parallelism due to issue on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chargesRDD = sc.textFile(urn, partitionNumber)\n",
    "\n",
    "#Get a new RDD with map function and lambda keyword. Remove header row.\n",
    "header = chargesRDD.take(1)[0]\n",
    "chargesRDD = chargesRDD.filter(lambda line: line!=header)\n",
    "chargesRDDSplit = chargesRDD.map(lambda line: line.replace('\"', '').split(','))\n",
    "chargesRDDSplit.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test data\n",
    "\n",
    "Mo knows where it's at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chargesRDDTEST = sc.textFile(urnTest, partitionNumber)\n",
    "   \n",
    "chargesRDDTEST = chargesRDD.filter(lambda line: line!=header)\n",
    "chargesRDDSplitTEST = chargesRDD.map(lambda line: line.replace('\"', '').split(','))\n",
    "chargesRDDSplitTEST.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training records is 279808.\n",
      "The number of test records is 279808\n",
      "First row of train:\n",
      "[u'\"HdBgCT1YkEl14280\",\"SHY\",436,\"I\",77,\"W\",\"M\",\"MS\",\"10/07/2014\",\"10/10/2014\",3,\"197.7\",\"SECOND MALIG NEO LIVE\",\"1\",10028']\n",
      "The first row of test:\n",
      "[u'\"HdBgCT1YkEl14280\",\"SHY\",436,\"I\",77,\"W\",\"M\",\"MS\",\"10/07/2014\",\"10/10/2014\",3,\"197.7\",\"SECOND MALIG NEO LIVE\",\"1\",10028']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#look at the data\n",
    "print ('The number of training records is %s.\\nThe number of test records is %s' % \n",
    "       (chargesRDD.count(), chargesRDDTEST.count()))\n",
    "print ('First row of train:\\n%s\\nThe first row of test:\\n%s\\n' % (chargesRDD.take(1), chargesRDDTEST.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VisitID - Identifier for patient visit.\n",
    "* Hospital - Admitting hospital.\n",
    "* Dept_Code - department code.\n",
    "* PaymentType - I am guessing a payment type for visit.\n",
    "* Age - Age of the patient in years.\n",
    "* Race - De-identified race of the patient.\n",
    "* Gender - Gender (\"M\" - male, \"F\" - female)\n",
    "* FC - ?\n",
    "* ArriveDate - Date of admission. \n",
    "* DischargeDate - Date of discharge\n",
    "* LOS - length of patient stay in days.\n",
    "* DXCODE - Diagnosis code.\n",
    "* Description - Description of diagnosis\n",
    "* DispenseID - ?\n",
    "* DOC - ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Random Forest Model\n",
    "### We want to predict the length of stay (LOS) given the patient demographics, dxcode and deptcode, and day of the week admitted.\n",
    "\n",
    "Assumptions of feature relevance...\n",
    "* Dept_code categorical, some departments would have more serious patients than others\n",
    "* Day of the week, Patients admitted over weekend may require a longer length of stay to be seen by necessary staff\n",
    "* Dxcode, multiple per patient, may need to use PCA to reduce number (PCA is not always good before random forest, see references)\n",
    "* Demographics: age, gender and race \n",
    "\n",
    "\n",
    "We go through the following pipeline:\n",
    "* Encode/Extract features\n",
    "* Train the model\n",
    "* Evaluate the model on unseen data\n",
    "* Draw conclusions and make recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not need to normalize the features for Random Forest, but bucketing may be needed to help with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dxcodeRDD = chargesRDDSplit.map(lambda line: (line[0], line[11])).groupByKey().distinct().cache()\n",
    "dxCodes = dxcodeRDD.values().flatMap(list).distinct()\n",
    "dxCodeCount = dxCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge(x, y):\n",
    "    \n",
    "    if x is not None: \n",
    "        x['dxCount'] += 1\n",
    "        if 'dxcode' in x:\n",
    "            x['dxcode'] = list(set(x['dxcode'] + y['dxcode']))\n",
    "        else:\n",
    "            x = y\n",
    "    else:\n",
    "        x = y\n",
    "    return x\n",
    "\n",
    "\n",
    "def mapAndFold(rdd):\n",
    "    return rdd.map(lambda line: (line[0], Row(\n",
    "             los=float(line[10]),\n",
    "             age=int(line[4]),\n",
    "             hospital_visit=line[1],\n",
    "             dept_code=line[2],\n",
    "             race=line[5], \n",
    "             gender_female=1 if line[6]=='F' else 0,    #Encode gender as boolean\n",
    "             dxcode=[line[11]],\n",
    "             admit_day=parse(line[8]).weekday(),\n",
    "             admit_month=parse(line[8]).month,\n",
    "             dxCount=1,\n",
    "             fc=line[7]\n",
    "            ))).foldByKey(None, merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.py:259: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n",
      "You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\n: java.lang.RuntimeException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:218)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:462)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)\r\n\tat org.apache.spark.sql.UDFRegistration.<init>(UDFRegistration.scala:40)\r\n\tat org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:330)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:214)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:808)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:791)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:582)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:557)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:599)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-10816166607d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mchargesByVisitRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapAndFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchargesRDDSplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchargesByVisitRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36m_ssql_ctx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_scala_HiveContext'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scala_HiveContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_hive_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scala_HiveContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36m_get_hive_ctx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_hive_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHiveContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefreshTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1064\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\n: java.lang.RuntimeException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:218)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:462)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)\r\n\tat org.apache.spark.sql.UDFRegistration.<init>(UDFRegistration.scala:40)\r\n\tat org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:330)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:214)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:808)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:791)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:582)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:557)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:599)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "chargesByVisitRDD = mapAndFold(chargesRDDSplit)\n",
    "#df = chargesByVisitRDD.values().toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'nizzRT/Dabr94244',\n",
       "  {'admit_day': 0,\n",
       "   'admit_month': 9,\n",
       "   'age': 64,\n",
       "   'dept_code': u'811',\n",
       "   'dxCount': 19,\n",
       "   'dxcode': [u'530.85',\n",
       "    u'276.2',\n",
       "    u'276.51',\n",
       "    u'272.4',\n",
       "    u'263.9',\n",
       "    u'414.01',\n",
       "    u'534.40',\n",
       "    u'285.1',\n",
       "    u'276.8',\n",
       "    u'537.4',\n",
       "    u'V12.71',\n",
       "    u'729.1',\n",
       "    u'327.23',\n",
       "    u'244.9',\n",
       "    u'496',\n",
       "    u'V12.55',\n",
       "    u'346.90',\n",
       "    u'530.81',\n",
       "    u'V12.51'],\n",
       "   'fc': u'UM',\n",
       "   'gender_female': 1,\n",
       "   'hospital_visit': u'MCH',\n",
       "   'los': 2.0,\n",
       "   'race': u'W'}),\n",
       " (u'afZQXxAQW+D+4218',\n",
       "  {'admit_day': 2,\n",
       "   'admit_month': 8,\n",
       "   'age': 31,\n",
       "   'dept_code': u'392',\n",
       "   'dxCount': 8,\n",
       "   'dxcode': [u'300.00',\n",
       "    u'112.1',\n",
       "    u'V12.55',\n",
       "    u'276.51',\n",
       "    u'338.29',\n",
       "    u'493.90',\n",
       "    u'787.01',\n",
       "    u'V44.2'],\n",
       "   'fc': u'M',\n",
       "   'gender_female': 1,\n",
       "   'hospital_visit': u'SHY',\n",
       "   'los': 2.0,\n",
       "   'race': u'W'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chargesByVisitRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\n: java.lang.RuntimeException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:218)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:462)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)\r\n\tat org.apache.spark.sql.UDFRegistration.<init>(UDFRegistration.scala:40)\r\n\tat org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:330)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:214)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:808)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:791)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:582)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:557)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:599)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-36b584aabb6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchargesByVisitRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36m_ssql_ctx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_scala_HiveContext'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scala_HiveContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_hive_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scala_HiveContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36m_get_hive_ctx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_hive_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHiveContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefreshTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1064\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-1.6.1-bin-hadoop2.6\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\n: java.lang.RuntimeException: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\r\n\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:204)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:238)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive$lzycompute(HiveContext.scala:218)\r\n\tat org.apache.spark.sql.hive.HiveContext.executionHive(HiveContext.scala:208)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:462)\r\n\tat org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:461)\r\n\tat org.apache.spark.sql.UDFRegistration.<init>(UDFRegistration.scala:40)\r\n\tat org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:330)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:90)\r\n\tat org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:101)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:214)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:808)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:791)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:582)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:557)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:599)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "df = chargesByVisitRDD.values().toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chargesByVisitRDDTEST = mapAndFold(chargesRDDSplitTEST)\n",
    "dfTEST = chargesByVisitRDD.values().toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexers = [\n",
    "    StringIndexer(inputCol=\"hospital_visit\", outputCol=\"hospitalIndex\"),\n",
    "    StringIndexer(inputCol=\"dept_code\", outputCol=\"deptIndex\"),\n",
    "    StringIndexer(inputCol=\"race\", outputCol=\"raceIndex\")\n",
    "]\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=2*dxCodeCount, inputCol=\"dxcode\", outputCol=\"dxCodes\")\n",
    "idf = IDF(inputCol=\"dxCodes\", outputCol=\"idfDxCodes\", minDocFreq=10)\n",
    "\n",
    "mungePipeline = Pipeline(stages=stringIndexers + [hashingTF, idf])\n",
    "\n",
    "mungingModel = mungePipeline.fit(df)\n",
    "trainingData = mungingModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testingData = mungingModel.transform(dfTEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munge it!\n",
    "String Indexers Need to have all values, so will need to fit combined traing and test data if unseen labels are present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "   inputCols=[\"age\", \n",
    "              \"deptIndex\", \n",
    "              \"gender_female\",\n",
    "              \"raceIndex\",\n",
    "              \"hospitalIndex\",\n",
    "              \"admit_day\",\n",
    "              \"admit_month\",\n",
    "              \"dxCount\",\n",
    "              \"idfDxCodes\"\n",
    "             ],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "transformedTrainingDF = assembler.transform(trainingData).select('features','los')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformedTestingDF = assembler.transform(testingData).select('features','los')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Now we generate the training and test data. We use the ***seed*** function to ensure a repeatable split of the data between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"los\",maxBins=1000, seed=1234)\n",
    "\n",
    "#Magic Numbers?\n",
    "rf.setNumTrees(100) \n",
    "rf.setMaxDepth(10) # Max of Spark is 30\n",
    "rf.setMinInstancesPerNode(5)\n",
    "rf.setFeatureSubsetStrategy('all')\n",
    "\n",
    "model = rf.fit(transformedTrainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "We evaluate the model on the unseen dataset that was not used to train the model. For reference here is the stats for los in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe('los').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "predictions = model.transform(transformedTestingDF).select('los','prediction')\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"los\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
